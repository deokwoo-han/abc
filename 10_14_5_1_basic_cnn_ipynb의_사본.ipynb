{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "10.14-5 1-basic-cnn.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deokwoo-han/abc/blob/master/10_14_5_1_basic_cnn_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKRc2pTYE6WX"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXQo4w5STPNE"
      },
      "source": [
        "#!cp ./drive/MyDrive/training_data.zip ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjFiphc8TFXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "441c1bb5-9cf6-4a53-ca9b-08e49214e882"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 610308\n",
            "-rw-r--r-- 1 root root 255872552 Oct 14 06:25 1-basic-cnn.h5\n",
            "-rw-r--r-- 1 root root 255876224 Oct 14 06:54 2-basic-dropout-cnn.h5\n",
            "drwx------ 5 root root      4096 Oct 14 05:26 drive\n",
            "drwxr-xr-x 1 root root      4096 Oct  8 13:45 sample_data\n",
            "drwxr-xr-x 2 root root     36864 Oct 14 05:27 test_data\n",
            "drwxr-xr-x 2 root root     94208 Oct 14 05:27 training_data\n",
            "-rw------- 1 root root 113023590 Oct 14 06:42 training_data.zip\n",
            "drwxr-xr-x 2 root root     36864 Oct 14 05:27 validation_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuH5s-ueFqgO"
      },
      "source": [
        "#!unzip training_data.zip #트레인 3000, 테스트, 배리데이션 1000개씩"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LymXXKhtE6Wu"
      },
      "source": [
        "import glob #라벨을 점으로 읽어들임, 구분\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LXg_kDaE6Wx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "96fe14a2-ef1a-47cd-ee75-9c4764e912a6"
      },
      "source": [
        "IMG_DIM = (150, 150)#굉장히 작은 사이즈의 이미지들이라서 150, 150픽셀로 정의\n",
        "\n",
        "train_files = glob.glob('training_data/*')\n",
        "train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]\n",
        "train_imgs = np.array(train_imgs)\n",
        "train_labels = [fn.split('/')[1].split('.')[0].strip() for fn in train_files]\n",
        "\n",
        "validation_files = glob.glob('validation_data/*')\n",
        "validation_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in validation_files]\n",
        "validation_imgs = np.array(validation_imgs)\n",
        "validation_labels = [fn.split('/')[1].split('.')[0].strip() for fn in validation_files]\n",
        "\n",
        "print('Train dataset shape:', train_imgs.shape, \n",
        "      '\\tValidation dataset shape:', validation_imgs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-244e742158d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_data/*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMG_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-244e742158d8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_data/*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMG_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    312\u001b[0m   \"\"\"\n\u001b[1;32m    313\u001b[0m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0;32m--> 314\u001b[0;31m                         target_size=target_size, interpolation=interpolation)\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \", \".join(_PIL_INTERPOLATION_METHODS.keys())))\n\u001b[1;32m    137\u001b[0m                 \u001b[0mresample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_PIL_INTERPOLATION_METHODS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth_height_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   1884\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1886\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreducing_gap\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresample\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mNEAREST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q7JpaBkSqGg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpGYIMjWE6Wz"
      },
      "source": [
        "train_imgs_scaled = train_imgs.astype('float32')\n",
        "validation_imgs_scaled  = validation_imgs.astype('float32')\n",
        "train_imgs_scaled /= 255\n",
        "validation_imgs_scaled /= 255#스케일 노멀라이즈, 소수점 값으로"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFW1_W3mTRWF"
      },
      "source": [
        "print(train_imgs[-1].shape)\n",
        "array_to_img(train_imgs[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOT1PSAETVPQ"
      },
      "source": [
        "print(train_imgs[0].shape)\n",
        "array_to_img(train_imgs[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-NzgUweTYFq"
      },
      "source": [
        "print(train_imgs[1500].shape)\n",
        "array_to_img(train_imgs[1500])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdBdehU4E6W0"
      },
      "source": [
        "print(train_imgs[99].shape)\n",
        "array_to_img(train_imgs[99])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYXSRvl2E6W1"
      },
      "source": [
        "batch_size = 50\n",
        "num_classes = 2\n",
        "epochs = 100 #150\n",
        "input_shape = (150, 150, 3)#클래스는 두개\n",
        "\n",
        "# encode text category labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()#0과 1 값으로\n",
        "le.fit(train_labels)\n",
        "train_labels_enc = le.transform(train_labels)\n",
        "validation_labels_enc = le.transform(validation_labels)#10개 추출\n",
        "\n",
        "print(train_labels[1495:1505], train_labels_enc[1495:1505])#라벨 인코딩 개와 고양이가 0과 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2NV3vFxqODk"
      },
      "source": [
        "### 이미지 증강(인위적으로 데이터를 3배 정도로 늘림)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr-hHWfwqV36"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOGrM6eGqFAB"
      },
      "source": [
        "train_data_gen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=True, rescale=1./225\n",
        "    )#버티컬은 꼬리와 입이 좌우로 바뀔 수 있으니... 스케일을 시켰음 증강"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXc2bmMzu37M"
      },
      "source": [
        "val_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfgkwAXxrpZ0"
      },
      "source": [
        "train_datagen.fit(train_imgs_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X54_GbHbvaHs"
      },
      "source": [
        "val_datagen.fit(validation_imgs_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D486LhDbsRw_"
      },
      "source": [
        "type(train_datagen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhTKc-35E6W3"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu6j2m8IE6W4"
      },
      "source": [
        "### Model Case I "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlAkT0L6E6W5"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', padding=\"same\",\n",
        "                 input_shape=input_shape))#모델 쉐이프가 150, 150, 3 이 됨\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))#첫 필터 사이즈는 16\n",
        "#다음에 들어가는 인풋 사이즈? 75 75\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#37이 나올것\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding=\"same\"))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "#18? \n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.3))#코드 4개 추가#배치 노말라이즈는 딱히... 패널티도 별로 효과가 없을...\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))#안의 파라미터의 값을 작은 수로 만들어 넣어야\n",
        "#아니면 너무 커서 오래 걸림\n",
        "#시그모이드 외에 소프트맥스도\n",
        "\n",
        "#오버피팅 해결하는 드랍 아웃"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBalCPmdE6W6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoqCOiFpE6W6"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer= 'ADAM', # optimizers.RMSprop(lr=0.0001)\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#model.summary()\n",
        "#...\n",
        "#..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLQgJU870kS-"
      },
      "source": [
        "train_generator = train_datagen.flow(train_imgs, train_labels_enc, batch_size=32)\n",
        "val_generator = val_datagen.flow(validation_imgs, validation_labels_enc, batch_size=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI097iPtsqxL"
      },
      "source": [
        "history = model.fit_generator(train_generator,  epochs=epochs,\n",
        "                              validation_data=val_generator,  verbose=1)\n",
        "                    #steps_per_epoch=len(train_imgs) / 32, \n",
        "#validation "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFagvQ3sE6W7"
      },
      "source": [
        "#model.summary()#에포크 100개 #모델의 갯수만 확인할 것, 이해, 피쳐 러닝, 500초"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fca-9VQE6W7"
      },
      "source": [
        "#history = model.fit(x=train_imgs_scaled, y=train_labels_enc,\n",
        "#                    validation_data=(validation_imgs_scaled, validation_labels_enc),\n",
        "#                    batch_size=batch_size,\n",
        "#                    epochs=epochs,\n",
        "#                    verbose=1)\n",
        "#                    #드랍 아웃 때문에 1초 늘어남\n",
        "#                    #다시 하늘로 솟구치며 드랍아웃도 별 의미가 없는"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFC4VgL-E6W8"
      },
      "source": [
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))#거의 답을 외워버림, 근데 validation은...\n",
        "t = f.suptitle('Basic CNN Performance', fontsize=12)#퍼포먼스 이름도 바꿨어야\n",
        "f.subplots_adjust(top=0.85, wspace=0.3)\n",
        "\n",
        "epoch_list = list(range(1,101))#151\n",
        "ax1.plot(epoch_list, history.history['accuracy'], label='Accuracy')#라벨 값이 다름 오류\n",
        "ax1.plot(epoch_list, history.history['val_accuracy'], label='Val_Accuracy')\n",
        "ax1.set_xticks(np.arange(0, 101, 5))#151\n",
        "ax1.set_ylabel('Accuracy Value')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_title('Accuracy')\n",
        "l1 = ax1.legend(loc=\"best\")\n",
        "\n",
        "ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n",
        "ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n",
        "ax2.set_xticks(np.arange(0, 100, 5))#151\n",
        "ax2.set_ylabel('Loss Value')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_title('Loss')\n",
        "l2 = ax2.legend(loc=\"best\")#심각한 오버피팅"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3TpE8iOE6W-"
      },
      "source": [
        "#model.save('1-basic-cnn.h5')\n",
        "#model.save('2-basic-dropout-cnn.h5')\n",
        "model.save('3-Aug-cnn.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHvABJIME6W_"
      },
      "source": [
        "#카피가 더 빠름\n",
        "#!cp ./1-basic-cnn.h5 /content/drive/MyDrive/kosa-2021"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWak05_OFYCn"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive') #맨 처음"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKyNDCrb0v62"
      },
      "source": [
        "https://github.com/JSJeong-me/CNN-Cats-Dogs/blob/main/3-aug_cnn-1014.ipynb"
      ]
    }
  ]
}