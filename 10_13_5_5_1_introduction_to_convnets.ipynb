{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "10.13-5 5.1-introduction-to-convnets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deokwoo-han/abc/blob/master/10_13_5_5_1_introduction_to_convnets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ga4jFFtoxYG1",
        "outputId": "637b7918-e026-49f7-fda2-3ac8bc2e4823"
      },
      "source": [
        "!pip install -r ./requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting absl-py==0.13.0\n",
            "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting asn1crypto==1.3.0\n",
            "  Downloading asn1crypto-1.3.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 43.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor==0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 3)) (0.8.1)\n",
            "Requirement already satisfied: cached-property==1.5.2 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 4)) (1.5.2)\n",
            "Requirement already satisfied: certifi==2021.5.30 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 5)) (2021.5.30)\n",
            "Collecting cffi==1.14.0\n",
            "  Downloading cffi-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (400 kB)\n",
            "\u001b[K     |████████████████████████████████| 400 kB 34.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 7)) (3.0.4)\n",
            "Collecting coverage==5.5\n",
            "  Downloading coverage-5.5-cp37-cp37m-manylinux2010_x86_64.whl (242 kB)\n",
            "\u001b[K     |████████████████████████████████| 242 kB 49.2 MB/s \n",
            "\u001b[?25hCollecting cryptography==2.8\n",
            "  Downloading cryptography-2.8-cp34-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 32.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython==0.29.24 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 10)) (0.29.24)\n",
            "Collecting gast==0.5.2\n",
            "  Downloading gast-0.5.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 12)) (0.2.0)\n",
            "Collecting grpcio==1.36.1\n",
            "  Downloading grpcio-1.36.1-cp37-cp37m-manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 37.8 MB/s \n",
            "\u001b[?25hCollecting h5py==3.2.1\n",
            "  Downloading h5py-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 36.3 MB/s \n",
            "\u001b[?25hCollecting idna==2.8\n",
            "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata==4.8.1 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 16)) (4.8.1)\n",
            "Collecting Keras==2.2.4\n",
            "  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 51.1 MB/s \n",
            "\u001b[?25hCollecting Keras-Applications==1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Keras-Preprocessing==1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 19)) (1.1.2)\n",
            "Requirement already satisfied: Markdown==3.3.4 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 20)) (3.3.4)\n",
            "Collecting mkl-fft==1.3.0\n",
            "  Downloading mkl_fft-1.3.0-1-cp37-cp37m-manylinux2014_x86_64.whl (240 kB)\n",
            "\u001b[K     |████████████████████████████████| 240 kB 50.1 MB/s \n",
            "\u001b[?25hCollecting mkl-random==1.2.2\n",
            "  Downloading mkl_random-1.2.2-1-cp37-cp37m-manylinux2014_x86_64.whl (379 kB)\n",
            "\u001b[K     |████████████████████████████████| 379 kB 45.3 MB/s \n",
            "\u001b[?25hCollecting mkl-service==2.4.0\n",
            "  Downloading mkl_service-2.4.0-0-cp37-cp37m-manylinux2014_x86_64.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting numpy==1.20.3\n",
            "  Downloading numpy-1.20.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 183 kB/s \n",
            "\u001b[?25hCollecting protobuf==3.17.2\n",
            "  Downloading protobuf-3.17.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 43.1 MB/s \n",
            "\u001b[?25hCollecting pycosat==0.6.3\n",
            "  Downloading pycosat-0.6.3.zip (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting pycparser==2.19\n",
            "  Downloading pycparser-2.19.tar.gz (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 46.8 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL==19.1.0\n",
            "  Downloading pyOpenSSL-19.1.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PySocks==1.7.1 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 29)) (1.7.1)\n",
            "Collecting PyYAML==5.3.1\n",
            "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
            "\u001b[K     |████████████████████████████████| 269 kB 50.5 MB/s \n",
            "\u001b[?25hCollecting requests==2.22.0\n",
            "  Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting ruamel-yaml==0.15.87\n",
            "  Downloading ruamel.yaml-0.15.87-cp37-cp37m-manylinux1_x86_64.whl (659 kB)\n",
            "\u001b[K     |████████████████████████████████| 659 kB 52.8 MB/s \n",
            "\u001b[?25hCollecting scipy==1.7.1\n",
            "  Downloading scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.5 MB 55 kB/s \n",
            "\u001b[?25hCollecting six==1.14.0\n",
            "  Downloading six-1.14.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting tensorboard==1.14.0\n",
            "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 47.5 MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.14.0\n",
            "  Downloading tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 109.3 MB 15 kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.14.0\n",
            "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
            "\u001b[K     |████████████████████████████████| 488 kB 46.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 38)) (1.1.0)\n",
            "Collecting typing-extensions==3.10.0.2\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata==4.8.1->-r ./requirements.txt (line 16)) (3.6.0)\n",
            "Collecting dpcpp_cpp_rt\n",
            "  Downloading dpcpp_cpp_rt-2021.4.0-py2.py3-none-manylinux1_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 36.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: mkl in /usr/local/lib/python3.7/dist-packages (from mkl-fft==1.3.0->-r ./requirements.txt (line 21)) (2019.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->-r ./requirements.txt (line 31)) (1.24.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard==1.14.0->-r ./requirements.txt (line 35)) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==1.14.0->-r ./requirements.txt (line 35)) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard==1.14.0->-r ./requirements.txt (line 35)) (0.37.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r ./requirements.txt (line 36)) (1.12.1)\n",
            "Collecting intel-opencl-rt==2021.4.0\n",
            "  Downloading intel_opencl_rt-2021.4.0-py2.py3-none-manylinux1_x86_64.whl (165.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 165.9 MB 63 kB/s \n",
            "\u001b[?25hCollecting intel-cmplr-lic-rt==2021.4.0\n",
            "  Downloading intel_cmplr_lic_rt-2021.4.0-py2.py3-none-manylinux1_x86_64.whl (24 kB)\n",
            "Collecting intel-cmplr-lib-rt==2021.4.0\n",
            "  Downloading intel_cmplr_lib_rt-2021.4.0-py2.py3-none-manylinux1_x86_64.whl (34.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.5 MB 72 kB/s \n",
            "\u001b[?25hRequirement already satisfied: intel-openmp==2021.4.0 in /usr/local/lib/python3.7/dist-packages (from dpcpp_cpp_rt->mkl-fft==1.3.0->-r ./requirements.txt (line 21)) (2021.4.0)\n",
            "Collecting tbb==2021.*\n",
            "  Downloading tbb-2021.4.0-py2.py3-none-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 41.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pycosat, pycparser, PyYAML\n",
            "  Building wheel for pycosat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycosat: filename=pycosat-0.6.3-cp37-cp37m-linux_x86_64.whl size=143873 sha256=2240d52ee1bd7e4d0f7a6b9ef64da9df272ff585120938f0b11efadb3a064d40\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/29/0e/a226b7c5a4f24e246f25201c0b1fbf6fb8f19f9cce89b02c36\n",
            "  Building wheel for pycparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycparser: filename=pycparser-2.19-py2.py3-none-any.whl size=111051 sha256=af335f426d1d9dfcb0df5f02a06dd72a463ad45fb1aa7702ab62a39a989f5a09\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/53/27/fee7fd9562e10dd046caccfc0340b8cf789b46846e660f3380\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44635 sha256=f513901fee6a0e7044f3ecac599c2acdba7c47829e9c22b8376ad568115c4331\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653\n",
            "Successfully built pycosat pycparser PyYAML\n",
            "Installing collected packages: typing-extensions, tbb, six, pycparser, numpy, intel-cmplr-lic-rt, protobuf, intel-opencl-rt, intel-cmplr-lib-rt, h5py, grpcio, cffi, absl-py, tensorflow-estimator, tensorboard, scipy, PyYAML, Keras-Applications, idna, gast, dpcpp-cpp-rt, cryptography, tensorflow, ruamel-yaml, requests, pyOpenSSL, pycosat, mkl-service, mkl-random, mkl-fft, Keras, coverage, asn1crypto\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.7.4.3\n",
            "    Uninstalling typing-extensions-3.7.4.3:\n",
            "      Successfully uninstalled typing-extensions-3.7.4.3\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 2.20\n",
            "    Uninstalling pycparser-2.20:\n",
            "      Successfully uninstalled pycparser-2.20\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.41.0\n",
            "    Uninstalling grpcio-1.41.0:\n",
            "      Successfully uninstalled grpcio-1.41.0\n",
            "  Attempting uninstall: cffi\n",
            "    Found existing installation: cffi 1.14.6\n",
            "    Uninstalling cffi-1.14.6:\n",
            "      Successfully uninstalled cffi-1.14.6\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 0.12.0\n",
            "    Uninstalling absl-py-0.12.0:\n",
            "      Successfully uninstalled absl-py-0.12.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.6.0\n",
            "    Uninstalling tensorboard-2.6.0:\n",
            "      Successfully uninstalled tensorboard-2.6.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.6.0\n",
            "    Uninstalling tensorflow-2.6.0:\n",
            "      Successfully uninstalled tensorflow-2.6.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: Keras\n",
            "    Found existing installation: keras 2.6.0\n",
            "    Uninstalling keras-2.6.0:\n",
            "      Successfully uninstalled keras-2.6.0\n",
            "  Attempting uninstall: coverage\n",
            "    Found existing installation: coverage 3.7.1\n",
            "    Uninstalling coverage-3.7.1:\n",
            "      Successfully uninstalled coverage-3.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.2.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.13.0 which is incompatible.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.14.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.22.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.14.0 which is incompatible.\n",
            "datascience 0.10.6 requires coverage==3.7.1, but you have coverage 5.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "coveralls 0.5 requires coverage<3.999,>=3.6, but you have coverage 5.5 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Keras-2.2.4 Keras-Applications-1.0.8 PyYAML-5.3.1 absl-py-0.13.0 asn1crypto-1.3.0 cffi-1.14.0 coverage-5.5 cryptography-2.8 dpcpp-cpp-rt-2021.4.0 gast-0.5.2 grpcio-1.36.1 h5py-3.2.1 idna-2.8 intel-cmplr-lib-rt-2021.4.0 intel-cmplr-lic-rt-2021.4.0 intel-opencl-rt-2021.4.0 mkl-fft-1.3.0 mkl-random-1.2.2 mkl-service-2.4.0 numpy-1.20.3 protobuf-3.17.2 pyOpenSSL-19.1.0 pycosat-0.6.3 pycparser-2.19 requests-2.22.0 ruamel-yaml-0.15.87 scipy-1.7.1 six-1.14.0 tbb-2021.4.0 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 typing-extensions-3.10.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cffi",
                  "google",
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "SyJjlwKSxLjl",
        "outputId": "1cfb587a-b2bf-4213-af8d-43e89b47d3de"
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.2.4'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "1T2s0ownxLjs"
      },
      "source": [
        "# 5.1 - 합성곱 신경망 소개\n",
        "\n",
        "이 노트북은 [케라스 창시자에게 배우는 딥러닝](https://tensorflow.blog/케라스-창시자에게-배우는-딥러닝/) 책의 5장 1절의 코드 예제입니다. 책에는 더 많은 내용과 그림이 있습니다. 이 노트북에는 소스 코드에 관련된 설명만 포함합니다. 이 노트북의 설명은 케라스 버전 2.2.2에 맞추어져 있습니다. 케라스 최신 버전이 릴리스되면 노트북을 다시 테스트하기 때문에 설명과 코드의 결과가 조금 다를 수 있습니다.\n",
        "\n",
        "----\n",
        "\n",
        "컨브넷의 정의와 컨브넷이 컴퓨터 비전 관련 작업에 잘 맞는 이유에 대해 이론적 배경을 알아보겠습니다. 하지만 먼저 간단한 컨브넷 예제를 둘러 보죠. 2장에서 완전 연결 네트워크로 풀었던(이 방식의 테스트 정확도는 97.8%였습니다) MNIST 숫자 이미지 분류에 컨브넷을 사용해 보겠습니다. 기본적인 컨브넷이더라도 2장의 완전 연결된 모델의 성능을 훨씬 앞지를 것입니다.\n",
        "\n",
        "다음 코드는 기본적인 컨브넷의 모습입니다. `Conv2D`와 `MaxPooling2D` 층을 쌓아 올렸습니다. 잠시 후에 이들이 무엇인지 배우겠습니다.\n",
        "\n",
        "컨브넷이 `(image_height, image_width, image_channels)` 크기의 입력 텐서를 사용한다는 점이 중요합니다(배치 차원은 포함하지 않습니다). 이 예제에서는 MNIST 이미지 포맷인 `(28, 28, 1)` 크기의 입력을 처리하도록 컨브넷을 설정해야 합니다. 이 때문에 첫 번째 층의 매개변수로 `input_shape=(28, 28, 1)`을 전달합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RovC7PTqxLjv",
        "outputId": "231a3b73-d0bc-4bf9-ece4-672e42d2d8d6"
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhk18k_qxLjw"
      },
      "source": [
        "지금까지 컨브넷 구조를 출력해 보죠:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvSfMlaIxLjx",
        "outputId": "5838faf1-1c08-42e7-f356-3ee6897ce255"
      },
      "source": [
        "model.summary()#26으로 줄어드는 것을 방지해주는 것이 패딩\n",
        "#패딩을 안주면 양 끝의 것이 없어짐 원본 셸을 붙여줌"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "=================================================================\n",
            "Total params: 55,744\n",
            "Trainable params: 55,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "swTRZAHpxLjx"
      },
      "source": [
        "`Conv2D`와 `MaxPooling2D` 층의 출력은 `(height, width, channels)` 크기의 3D 텐서입니다. 높이와 넓이 차원은 네트워크가 깊어질수록 작아지는 경향이 있습니다. 채널의 수는 `Conv2D` 층에 전달된 첫 번째 매개변수에 의해 조절됩니다(32개 또는 64개).\n",
        "\n",
        "다음 단계에서 마지막 층의 (`(3, 3, 64)` 크기인) 출력 텐서를 완전 연결 네트워크에 주입합니다. 이 네트워크는 이미 익숙하게 보았던 `Dense` 층을 쌓은 분류기입니다. 이 분류기는 1D 벡터를 처리하는데 이전 층의 출력이 3D 텐서입니다. 그래서 먼저 3D 출력을 1D 텐서로 펼쳐야 합니다. 그다음 몇 개의 `Dense` 층을 추가합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RkbaDX9xLjy"
      },
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E6pts9MxLjz"
      },
      "source": [
        "10개의 클래스를 분류하기 위해 마지막 층의 출력 크기를 10으로 하고 소프트맥스 활성화 함수를 사용합니다. 이제 전체 네트워크는 다음과 같습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UysQmfsTxLj0",
        "outputId": "71073a1c-7147-4293-cb74-f1f55e9c0412"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                200768    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 257,162\n",
            "Trainable params: 257,162\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjp-G9x1xLj1"
      },
      "source": [
        "여기에서 볼 수 있듯이 `(3, 3, 64)` 출력이 `(576,)` 크기의 벡터로 펼쳐진 후 `Dense` 층으로 주입되었습니다.\n",
        "\n",
        "이제 MNIST 숫자 이미지에 이 컨브넷을 훈련합니다. 2장의 MNIST 예제 코드를 많이 재사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyDHHe-AxLj1",
        "outputId": "3acff056-d50b-4a2b-c762-e937e57f0fde"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlQIQSQixLj2",
        "outputId": "0513ac13-3e62-4272-f86b-7d34d31ad37a"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 86s 1ms/step - loss: 0.1481 - acc: 0.9536\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 87s 1ms/step - loss: 0.0420 - acc: 0.9874\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 86s 1ms/step - loss: 0.0286 - acc: 0.9909\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 88s 1ms/step - loss: 0.0218 - acc: 0.9932\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 85s 1ms/step - loss: 0.0170 - acc: 0.9949\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f70dd4db650>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcNn7zT3xLj3"
      },
      "source": [
        "테스트 데이터에서 모델을 평가해 보죠:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfx7X7s6xLj3",
        "outputId": "0faec25e-d0cc-4206-b4b3-4c23e6eb9ea6"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 4s 442us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUTuCALexLj4",
        "outputId": "8eba92de-efce-444d-99e0-f15e313b0b08"
      },
      "source": [
        "test_acc"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9891"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yF1tntaxLj4"
      },
      "source": [
        "2장의 완전 연결 네트워크는 97.8%의 테스트 정확도를 얻은 반면, 기본적인 컨브넷은 99.2%의 테스트 정확도를 얻었습니다. 에러율이 (상대적으로) 64%나 줄었습니다. 나쁘지 않군요!"
      ]
    }
  ]
}